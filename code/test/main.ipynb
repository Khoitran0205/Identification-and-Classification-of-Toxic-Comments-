{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk \n",
    "import numpy as np\n",
    "from text_processing import TextProcessor\n",
    "from text_vectorize import TextVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pk.load(open('D:/Identification-and-Classification-of-Toxic-Comments-/code/model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_demo = input(\"Enter a text: \")\n",
    "text_processing = TextProcessor()\n",
    "text_token = text_processing.process(text_demo)\n",
    "text_vectorizer = TextVectorizer()\n",
    "arr_text  = text_vectorizer.process(text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_predict = model.predict(np.reshape(arr_text,(1,-1)))\n",
    "for i in range(len(LIST_LABELS)):\n",
    "    print(LIST_LABELS[i], ':', result_predict[0][i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
